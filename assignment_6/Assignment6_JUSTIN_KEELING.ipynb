{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment6_FNAME_LNAME.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LqQA2skLfOQ",
        "colab_type": "text"
      },
      "source": [
        "## Classifying Protein-function comentions\n",
        "\n",
        "A “*comention*” is defined as a co-occurrence of two Named Enitities (NEs) within a short span of text. Here, the two NEs we are interested are proteins and functions. The span of text is a sentence. In this assignment, you will write a python module that loads classifies sentence-level protein-function comentions.\n",
        "\n",
        "Your first task is to load comention data stored in a `TRMTR2-train-small.txt` file. All these annotations were done manually from automatically tagged proteins (Uniprot identifiers) and functions (Gene Ontology concepts). During annotation, the experts were concerned with the correctness of the relationship between the entities. The labels are of type \"GoodComention\" if the sentence suggests that the protein has some relationship with the function. Otherwise the comention is labeld as \"BadComention\". In order to determine this lable, no other information (other than meaning of the sentence) is used.\n",
        "\n",
        "The file is formatted in the way described below. More specifically, it is what's called a tab-delimited file. Here is a line of our example file:\n",
        "\n",
        "`24002896        PROT (suppressor of cytokine signaling 1), not only has an effect on cytokine signaling pathway, it is also involved in the FUNC pathway in cell apoptosis.    O15524     SOCS1   GO:0009966      regulation of signal transduction     GoodComention`\n",
        "\n",
        "Each line has the format:\n",
        "\n",
        "`articleID \\t sentenceText \\t UniprotID \\t proteinTextMatched \\t GOID \\t GOTextMatched \\t [GoodComention/BadComention]`\n",
        "\n",
        "This indicates that protein identifed by `UniprotID` is comentioned with the function identified by `GOID` within a sentence. If the label is GoodComention means that this comention was evaluated by a human curator as a valid functional relationship between the protein and the functional category (i.e. not all comentions are valid). In writing your code, use the **“TRMTR2-train-small.txt”** which is related to the category `GO:0022857 (transmembrane transporter activity)`. This file contains 284 labeled protein-function comentions. \n",
        "\n",
        "Within `sentenceText`: \n",
        " - the `proteinTextMatched` is replaced with \"PROT\"\n",
        " - the `GOTextMatched` is replaced with \"FUNC\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udRKYlnuNVhv",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: Baseline machine learing model\n",
        "\n",
        "Develop a supervised machine learning model using bag-of-words or TFIDF features. Use `cross_validate()` to evaluate the performance of your model using 5-fold cross validation with your preffered learing algorithm. Show the P, R, F1 and AUROC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wP2ZTx6s1J5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you can use this for removing non-ascii charaters from text\n",
        "def remove_non_ascii(text):\n",
        "    return re.sub(r'[^\\x00-\\x7F]', ' ', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IxhRWNSwhZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Write your code here for reading the files (you can use the csv library: https://docs.python.org/3/library/csv.html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jaVdYwkwsLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Write your code here for cross-validation (use cross_validate())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlOm7eL32sb8",
        "colab_type": "text"
      },
      "source": [
        "1. How did you preprocess the sentences (remove any special charaters? remove stop words? Apply stemming/ lemmatization etc?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8357fLV2wlF",
        "colab_type": "text"
      },
      "source": [
        "2. Which ML algorithm did you use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI1Nua4zxJmp",
        "colab_type": "text"
      },
      "source": [
        "3. List P, R, F1 and AUROC from your baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6MtdYEZtjhl",
        "colab_type": "text"
      },
      "source": [
        "## Part 2: Advanced machine learning model\n",
        "\n",
        "Develop an advanced supervised machine learning model by developing a collection of features (enitity-based, word-based, syntactic). Use cross_validate() to evaluate the performance of your advanced model using 5-fold cross validation with your preffered learning algorithm (used for part 1). Show the P, R, F1 and AUROC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA6iSnyPyCAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here for generating (at least 3) enitity-based features of your choice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgGcDxv_y9AS",
        "colab_type": "text"
      },
      "source": [
        "4. List your enity-based features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P69d8zWbyJn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here for generating (at least 3) word-based features of your choice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhGJOXrjzCMM",
        "colab_type": "text"
      },
      "source": [
        "5. List your word-based features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPha70KIfvTu",
        "colab_type": "text"
      },
      "source": [
        "In this exercise you will use the **length of the dependency path** (see \"Syntactic Features for Relation Extraction\" in Lecture 7 slides) as a syntactic feature.\n",
        "\n",
        "First, you will use [spaCy](https://spacy.io/) library, which is an indutrial-strength NLP library for python, for first finding the **dependency parse** of each sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhHzbZ-KwuRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download spacy and install spaCy model\n",
        "! pip install spacy\n",
        "! python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGuBmzUnjm4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#following code snippet shows how to generate the dependencies between tokens\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(u'Convulsions that occur after DTaP are caused by a fever.')\n",
        "\n",
        "#Prints the dependencies: parent tokens, child token and the type of the dependency \n",
        "for token in doc:\n",
        "  print((token.head.text, token.text, token.dep_))\n",
        "\n",
        "#You can also view the dependency structure by visiting this page: https://explosion.ai/demos/displacy?text=Convulsions%20that%20occur%20after%20DTaP%20are%20caused%20by%20a%20fever.&model=en_core_web_sm&cpu=1&cph=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOSUrwcJsILD",
        "colab_type": "text"
      },
      "source": [
        "Secondly, you will use [networkx](https://networkx.github.io/documentation/stable/index.html) library for generating **dependency parse graphs** using the above information obtained through spaCy. netwrokx is one of the most popular graph libraries for python. Then you will find the **shortest distance** between the two entities in this graph and use that as the feature value.\n",
        "\n",
        "**You can ignore the types and directionaly of the dependencies for the purposes of this assignment. In otherwords, assume all edges are undirected and are of the same type.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v5buUB5zr10K",
        "colab": {}
      },
      "source": [
        "#install networkx\n",
        "! pip install networkx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2LjRQymiyNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#You can generate graphs by follwoing the networkx tutorial: https://networkx.github.io/documentation/stable/tutorial.html\n",
        "#following code snippet shows you how to gerenarte a toy graph\n",
        "import networkx as nx\n",
        "G = nx.Graph()\n",
        "G.add_node('caused')\n",
        "G.add_node('Convulsions')\n",
        "print(list(G.nodes))\n",
        "\n",
        "G.add_edge('caused', 'Convulsions')\n",
        "print(list(G.edges))\n",
        "\n",
        "print(G.number_of_nodes())\n",
        "print(G.number_of_edges())\n",
        "\n",
        "# Get the path and its length using https://networkx.github.io/documentation/stable/reference/algorithms/shortest_paths.html\n",
        "entity1 = 'Convulsions'\n",
        "entity2 = 'caused'\n",
        "print(\"dependency path = \",nx.shortest_path(G, source=entity1, target=entity2))\n",
        "print(\"dependency path length = \", nx.shortest_path_length(G, source=entity1, target=entity2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-6KjlMritYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using the above spaCy/networkx copde snippets as the guide, write code here to generate dependency path lengths for all the comentions in train data. Add these values as the sytactic feature values."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0qGKhCSqX89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Write code to plot the distrubution of dependency path lengths using hist function: https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTa3u3eZyj43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Write your code here for cross-validation (use cross_validate())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DHO0grmHytdm"
      },
      "source": [
        "6. List P, R, F1 and AUROC from your advanced model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPSAPvTW6eZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Write code here to generate a bar chart (using matplotlib) comparing the P,R,F1, and AUROC values of the two models (baseline vs. advanced)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAdfDg6oQcZF",
        "colab_type": "text"
      },
      "source": [
        "## Part 3: Prediction\n",
        "\n",
        "The (unlabled) test data is given in `TRMTR2-test-unlabeled-small.txt`. Format of this file is as follows:\n",
        "\n",
        "`articleID \\t sentenceText \\t UniprotID \\t proteinTextMatched \\t GOID \\t GOTextMatched`\n",
        "\n",
        "Using your best model, write code predict labels for the test data. Then save your predictions to `TRMTR2-test-preds-small.txt`. Format of this file should **exactly** follow the training data format. **Upload this file in D2L file (along with the ipynb file).**\n",
        "\n",
        "We will use your generated \"`TRMTR2-test-preds-small.txt`\" file for evaluting the final performance of your code. Given that this is a class assignment, we are not expecting to see state-of-the-art performance. But you need to show that you have carefully though about the types of features that are informative for this task.\n",
        "\n",
        "The best performance on test data will get bonus points (5% for first, 3% for second, 2% for third) of the assignment grade. The leaderboad will be annouced in the class afther final evalution."
      ]
    }
  ]
}